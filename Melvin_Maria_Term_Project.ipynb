{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1- Build an RNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data and Preprocess\n",
    "\n",
    "with open(\"Ecoli_GCF_003018035.1_ASM301803v1_genomic.fna\", \"r\") as file:\n",
    "    data = file.read().splitlines()\n",
    "    genome_sequence = \"\".join([line for line in data if not line.startswith('>')])\n",
    "\n",
    "train_size = int(0.8 * len(genome_sequence))\n",
    "train_data = genome_sequence[:train_size]\n",
    "test_data = genome_sequence[train_size:]\n",
    "\n",
    "char_to_idx = {char: i for i, char in enumerate(sorted(set(train_data)))}\n",
    "idx_to_char = {i: char for char, i in char_to_idx.items()}\n",
    "\n",
    "def sequence_to_tensor(sequence, char_to_idx):\n",
    "    return torch.tensor([char_to_idx[char] for char in sequence], dtype=torch.long)\n",
    "\n",
    "train_data_tensor = sequence_to_tensor(train_data, char_to_idx)\n",
    "test_data_tensor = sequence_to_tensor(test_data, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define RNN model architecture\n",
    "class EcoliRNN(nn.Module):\n",
    "    def __init__(self, inut_size, hidden_size, output_size):\n",
    "        super(EcoliRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# Training parameters\n",
    "input_size = len(char_to_idx)\n",
    "hidden_size = 128\n",
    "output_size = len(char_to_idx)\n",
    "batch_size = 100\n",
    "seq_length = 50\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 2500\n",
    "\n",
    "# Instantiate the model\n",
    "model = EcoliRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and target sequences for training\n",
    "def random_training_example(train_data_tensor, seq_length):\n",
    "    start_idx = torch.randint(0, len(train_data_tensor) - seq_length - 1, (1,)).item()\n",
    "    input_sequence = train_data_tensor[start_idx:start_idx + seq_length]\n",
    "    target_sequence = train_data_tensor[start_idx + 1:start_idx + seq_length + 1]\n",
    "    return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.384377044677734\n",
      "Epoch 250: Loss 1.3544420898437501\n",
      "Epoch 500: Loss 1.3570636283874506\n",
      "Epoch 750: Loss 1.3541837188720702\n",
      "Epoch 1000: Loss 1.359039141845703\n",
      "Epoch 1250: Loss 1.352267694091797\n",
      "Epoch 1500: Loss 1.3538534706115724\n",
      "Epoch 1750: Loss 1.3561255218505857\n",
      "Epoch 2000: Loss 1.357295195007324\n",
      "Epoch 2250: Loss 1.3502745590209964\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "def one_hot_encode(sequence, vocab_size):\n",
    "    encoded = torch.zeros(len(sequence), vocab_size)\n",
    "    for i, idx in enumerate(sequence):\n",
    "        encoded[i, idx] = 1.0\n",
    "    return encoded\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for _ in range(batch_size):\n",
    "        input_sequence, target_sequence = random_training_example(train_data_tensor, seq_length)\n",
    "        input_sequence = one_hot_encode(input_sequence, input_size).unsqueeze(0)  # Add one-hot encoding and batch dimension\n",
    "        hidden = model.init_hidden()\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(input_sequence.size(1)):\n",
    "            output, hidden = model(input_sequence[:, i], hidden)\n",
    "            loss += criterion(output, target_sequence[i].unsqueeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() / seq_length\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss / batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sequence: CAGTAACTATAGGCAGAAGAGGTACGACGATGATCAGCAGCAGGGTCCCG\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset\n",
    "def evaluate(input_tensor, seq_length):\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden()\n",
    "        output_sequence = []\n",
    "\n",
    "        for i in range(input_tensor.size(1)):\n",
    "            output, hidden = model(input_tensor[:, i], hidden)\n",
    "            top_idx = output.argmax(1).item()\n",
    "            output_sequence.append(idx_to_char[top_idx])\n",
    "\n",
    "        return \"\".join(output_sequence)\n",
    "\n",
    "test_input, _ = random_training_example(test_data_tensor, seq_length)\n",
    "test_input = one_hot_encode(test_input, input_size).unsqueeze(0)  # Adding a one-hot encoding and batch dimension\n",
    "predicted_sequence = evaluate(test_input, seq_length)\n",
    "print(\"Predicted sequence:\", predicted_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy: 32.62%\n"
     ]
    }
   ],
   "source": [
    "# Modified evaluate function to return accuracy\n",
    "def evaluate(input_tensor, target_tensor):\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden()\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(input_tensor.size(1)):\n",
    "            output, hidden = model(input_tensor[:, i], hidden)\n",
    "            top_idx = output.argmax(1).item()\n",
    "            if top_idx == target_tensor[i].item():\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct / input_tensor.size(1)\n",
    "        return accuracy\n",
    "\n",
    "# Calculate average test accuracy\n",
    "num_test_examples = 500000\n",
    "total_accuracy = 0.0\n",
    "\n",
    "for _ in range(num_test_examples):\n",
    "    test_input, test_target = random_training_example(test_data_tensor, seq_length)\n",
    "    test_input = one_hot_encode(test_input, input_size).unsqueeze(0)\n",
    "    accuracy = evaluate(test_input, test_target)\n",
    "    total_accuracy += accuracy\n",
    "\n",
    "avg_test_accuracy = total_accuracy / num_test_examples\n",
    "print(f\"Average test accuracy: {avg_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Build a word-level model using RNN and Break a DNA sequence into k-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def generate_kmers(sequence, k):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "k = 4\n",
    "train_kmers = generate_kmers(train_data, k)\n",
    "test_kmers = generate_kmers(test_data, k)\n",
    "\n",
    "unique_kmers = sorted(set(train_kmers))\n",
    "kmer_to_idx = {kmer: i for i, kmer in enumerate(unique_kmers)}\n",
    "idx_to_kmer = {i: kmer for kmer, i in kmer_to_idx.items()}\n",
    "\n",
    "vocab_size = len(unique_kmers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Model\n",
    "class KmerRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(KmerRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluate\n",
    "import random\n",
    "\n",
    "def random_training_example(data, kmer_to_idx):\n",
    "    kmer_idx = random.randint(0, len(data) - 2)\n",
    "    input_kmer = data[kmer_idx]\n",
    "    target_kmer = data[kmer_idx + 1]\n",
    "    return torch.tensor([kmer_to_idx[input_kmer]], dtype=torch.long), torch.tensor([kmer_to_idx[target_kmer]], dtype=torch.long)\n",
    "\n",
    "def one_hot_encode(sequence, vocab_size):\n",
    "    encoded = torch.zeros(1, vocab_size)\n",
    "    for idx in sequence:\n",
    "        encoded[0, idx] = 1.0\n",
    "    return encoded\n",
    "\n",
    "def train(input_tensor, target_tensor, model, criterion, optimizer):\n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    output, hidden = model(input_tensor, hidden)\n",
    "    loss = criterion(output, target_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Training parameter\n",
    "input_size = vocab_size\n",
    "hidden_size = 128\n",
    "output_size = vocab_size\n",
    "\n",
    "model = KmerRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 5.547083258628845\n",
      "Epoch 250: Loss 3.6827583360671996\n",
      "Epoch 500: Loss 2.248655087351799\n",
      "Epoch 750: Loss 1.597973136305809\n",
      "Epoch 1000: Loss 1.5414057582616807\n",
      "Epoch 1250: Loss 1.426462857723236\n",
      "Epoch 1500: Loss 1.4123170167207717\n",
      "Epoch 1750: Loss 1.3980057382583617\n",
      "Epoch 2000: Loss 1.3772445595264435\n",
      "Epoch 2250: Loss 1.384626290202141\n",
      "Epoch 2500: Loss 1.4389013969898223\n",
      "Epoch 2750: Loss 1.3612875956296921\n",
      "Epoch 3000: Loss 1.3069980251789093\n",
      "Epoch 3250: Loss 1.418328297138214\n",
      "Epoch 3500: Loss 1.2573722660541535\n",
      "Epoch 3750: Loss 1.4146320909261703\n",
      "Epoch 4000: Loss 1.4376247972249985\n",
      "Epoch 4250: Loss 1.4022792196273803\n",
      "Epoch 4500: Loss 1.4348552185297012\n",
      "Epoch 4750: Loss 1.3971243649721146\n",
      "Epoch 5000: Loss 1.3482069471478462\n",
      "Epoch 5250: Loss 1.337926074564457\n",
      "Epoch 5500: Loss 1.3567278790473938\n",
      "Epoch 5750: Loss 1.3819846951961516\n",
      "Epoch 6000: Loss 1.332816428542137\n",
      "Epoch 6250: Loss 1.3065741264820099\n",
      "Epoch 6500: Loss 1.2985156771540642\n",
      "Epoch 6750: Loss 1.3082764089107513\n",
      "Epoch 7000: Loss 1.358717648088932\n",
      "Epoch 7250: Loss 1.3354121121764182\n",
      "Epoch 7500: Loss 1.3819658464193345\n",
      "Epoch 7750: Loss 1.3701412105560302\n",
      "Epoch 8000: Loss 1.389539595246315\n",
      "Epoch 8250: Loss 1.3593505877256393\n",
      "Epoch 8500: Loss 1.402619840502739\n",
      "Epoch 8750: Loss 1.4001094871759414\n",
      "Epoch 9000: Loss 1.3537101864814758\n",
      "Epoch 9250: Loss 1.4135778224468232\n",
      "Epoch 9500: Loss 1.362396137714386\n",
      "Epoch 9750: Loss 1.34243227571249\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for _ in range(batch_size):\n",
    "        input_kmer, target_kmer = random_training_example(train_kmers, kmer_to_idx)\n",
    "        input_tensor = one_hot_encode(input_kmer, input_size)\n",
    "        loss = train(input_tensor, target_kmer, model, criterion, optimizer)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss / batch_size}\")\n",
    "\n",
    "def test_model(input_tensor, target_tensor, model):\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden()\n",
    "        output, _ = model(input_tensor, hidden)\n",
    "        top_idx = output.argmax(1).item()\n",
    "\n",
    "        accuracy = 1 if top_idx == target_tensor.item() else 0\n",
    "        predicted_kmer = idx_to_kmer[top_idx]\n",
    "        actual_kmer = idx_to_kmer[target_tensor.item()]\n",
    "\n",
    "        return accuracy, predicted_kmer, actual_kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: ATCGAGGATC\n",
      "K-mers in the input sequence: ['ATCG', 'TCGA', 'CGAG', 'GAGG', 'AGGA', 'GGAT', 'GATC']\n",
      "Example 1:\n",
      "Input K-mer: ATCG\n",
      "Predicted K-mer: TCGC\n",
      "Actual K-mer: TCGA\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 2:\n",
      "Input K-mer: TCGA\n",
      "Predicted K-mer: CGAT\n",
      "Actual K-mer: CGAG\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 3:\n",
      "Input K-mer: CGAG\n",
      "Predicted K-mer: GAGA\n",
      "Actual K-mer: GAGG\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 4:\n",
      "Input K-mer: GAGG\n",
      "Predicted K-mer: AGGA\n",
      "Actual K-mer: AGGA\n",
      "Strict Accuracy: 100%\n",
      "Nuanced Accuracy: 100.0%\n",
      "\n",
      "Example 5:\n",
      "Input K-mer: AGGA\n",
      "Predicted K-mer: GGAA\n",
      "Actual K-mer: GGAT\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 6:\n",
      "Input K-mer: GGAT\n",
      "Predicted K-mer: GATG\n",
      "Actual K-mer: GATC\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Average strict test accuracy: 16.67%\n",
      "Average nuanced test accuracy: 79.17%\n"
     ]
    }
   ],
   "source": [
    "# define a Hamming distance between two strings s1 and s2 of equal length\n",
    "def hamming_distance(s1, s2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "\n",
    "def nuanced_accuracy(predicted_kmer, actual_kmer):\n",
    "    return 1 - (hamming_distance(predicted_kmer, actual_kmer) / len(predicted_kmer))\n",
    "\n",
    "input_seq = \"ATCGAGGATC\"\n",
    "kmers = generate_kmers(input_seq, k)\n",
    "\n",
    "print(\"Input sequence:\", input_seq)\n",
    "print(\"K-mers in the input sequence:\", kmers)\n",
    "\n",
    "total_accuracy = 0.0\n",
    "total_nuanced_accuracy = 0.0\n",
    "\n",
    "for i in range(len(kmers) - 1):\n",
    "    input_kmer = torch.tensor([kmer_to_idx[kmers[i]]], dtype=torch.long)\n",
    "    input_kmer_tensor = one_hot_encode(input_kmer, input_size)\n",
    "    target_kmer = torch.tensor([kmer_to_idx[kmers[i + 1]]], dtype=torch.long)\n",
    "\n",
    "    accuracy, predicted_kmer, actual_kmer = test_model(input_kmer_tensor, target_kmer, model)\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "    nuanced_acc = nuanced_accuracy(predicted_kmer, actual_kmer)\n",
    "    total_nuanced_accuracy += nuanced_acc\n",
    "\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"Input K-mer: {kmers[i]}\")\n",
    "    print(f\"Predicted K-mer: {predicted_kmer}\")\n",
    "    print(f\"Actual K-mer: {actual_kmer}\")\n",
    "    print(f\"Strict Accuracy: {accuracy * 100}%\")\n",
    "    print(f\"Nuanced Accuracy: {nuanced_acc * 100}%\\n\")\n",
    "\n",
    "avg_test_accuracy = total_accuracy / (len(kmers) - 1)\n",
    "avg_nuanced_test_accuracy = total_nuanced_accuracy / (len(kmers) - 1)\n",
    "print(f\"Average strict test accuracy: {avg_test_accuracy * 100:.2f}%\")\n",
    "print(f\"Average nuanced test accuracy: {avg_nuanced_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-RNN with Attention design to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class KmerRNNAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(KmerRNNAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = nn.Linear(hidden_size, input_size)  # Attention layer\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = F.softmax(self.attn(hidden), dim=1)\n",
    "        attn_applied = torch.mul(attn_weights, input)\n",
    "\n",
    "        output = self.i2o(attn_applied)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_example(data, kmer_to_idx):\n",
    "    kmer_idx = random.randint(0, len(data) - 2)\n",
    "    input_kmer = data[kmer_idx]\n",
    "    target_kmer = data[kmer_idx + 1]\n",
    "    return torch.tensor([kmer_to_idx[input_kmer]], dtype=torch.long), torch.tensor([kmer_to_idx[target_kmer]], dtype=torch.long)\n",
    "\n",
    "def one_hot_encode(sequence, vocab_size):\n",
    "    encoded = torch.zeros(1, vocab_size)\n",
    "    for idx in sequence:\n",
    "        encoded[0, idx] = 1.0\n",
    "    return encoded\n",
    "\n",
    "def train(input_tensor, target_tensor, model, criterion, optimizer):\n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    output, hidden = model(input_tensor, hidden)\n",
    "    loss = criterion(output, target_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Training parameter\n",
    "input_size = vocab_size\n",
    "hidden_size = 256\n",
    "output_size = vocab_size\n",
    "\n",
    "model = KmerRNNAttention(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 5.5569839811325075\n",
      "Epoch 250: Loss 3.4967409777641296\n",
      "Epoch 500: Loss 2.3014929807186126\n",
      "Epoch 750: Loss 1.6671909499168396\n",
      "Epoch 1000: Loss 1.5686285656690597\n",
      "Epoch 1250: Loss 1.336796681880951\n",
      "Epoch 1500: Loss 1.3385279452800751\n",
      "Epoch 1750: Loss 1.3203246515989304\n",
      "Epoch 2000: Loss 1.3593204373121262\n",
      "Epoch 2250: Loss 1.3141004472970963\n",
      "Epoch 2500: Loss 1.3457167062163353\n",
      "Epoch 2750: Loss 1.4145583122968675\n",
      "Epoch 3000: Loss 1.3378634890913963\n",
      "Epoch 3250: Loss 1.3982258999347688\n",
      "Epoch 3500: Loss 1.3437099224328994\n",
      "Epoch 3750: Loss 1.4484151017665863\n",
      "Epoch 4000: Loss 1.429296840429306\n",
      "Epoch 4250: Loss 1.4316371649503707\n",
      "Epoch 4500: Loss 1.4179669213294983\n",
      "Epoch 4750: Loss 1.3864009046554566\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for _ in range(batch_size):\n",
    "        input_kmer, target_kmer = random_training_example(train_kmers, kmer_to_idx)\n",
    "        input_tensor = one_hot_encode(input_kmer, input_size)\n",
    "        loss = train(input_tensor, target_kmer, model, criterion, optimizer)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss / batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: ATCGAGGATC\n",
      "K-mers in the input sequence: ['ATCG', 'TCGA', 'CGAG', 'GAGG', 'AGGA', 'GGAT', 'GATC']\n",
      "Example 1:\n",
      "Input K-mer: ATCG\n",
      "Predicted K-mer: TCGC\n",
      "Actual K-mer: TCGA\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 2:\n",
      "Input K-mer: TCGA\n",
      "Predicted K-mer: CGAT\n",
      "Actual K-mer: CGAG\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 3:\n",
      "Input K-mer: CGAG\n",
      "Predicted K-mer: GAGC\n",
      "Actual K-mer: GAGG\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 4:\n",
      "Input K-mer: GAGG\n",
      "Predicted K-mer: AGGA\n",
      "Actual K-mer: AGGA\n",
      "Strict Accuracy: 100%\n",
      "Nuanced Accuracy: 100.0%\n",
      "\n",
      "Example 5:\n",
      "Input K-mer: AGGA\n",
      "Predicted K-mer: GGAA\n",
      "Actual K-mer: GGAT\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Example 6:\n",
      "Input K-mer: GGAT\n",
      "Predicted K-mer: GATG\n",
      "Actual K-mer: GATC\n",
      "Strict Accuracy: 0%\n",
      "Nuanced Accuracy: 75.0%\n",
      "\n",
      "Average strict test accuracy: 16.67%\n",
      "Average nuanced test accuracy: 79.17%\n"
     ]
    }
   ],
   "source": [
    "#testing set \n",
    "\n",
    "def test_model_attention(input_tensor, target_tensor, model):\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden()\n",
    "        output, _ = model(input_tensor, hidden)\n",
    "        top_idx = output.argmax(1).item()\n",
    "\n",
    "        accuracy = 1 if top_idx == target_tensor.item() else 0\n",
    "        predicted_kmer = idx_to_kmer[top_idx]\n",
    "        actual_kmer = idx_to_kmer[target_tensor.item()]\n",
    "\n",
    "        return accuracy, predicted_kmer, actual_kmer\n",
    "\n",
    "def hamming_distance(s1, s2):\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "\n",
    "def nuanced_accuracy(predicted_kmer, actual_kmer):\n",
    "    return 1 - (hamming_distance(predicted_kmer, actual_kmer) / len(predicted_kmer))\n",
    "\n",
    "input_seq = \"ATCGAGGATC\"\n",
    "kmers = generate_kmers(input_seq, k)\n",
    "\n",
    "print(\"Input sequence:\", input_seq)\n",
    "print(\"K-mers in the input sequence:\", kmers)\n",
    "\n",
    "total_accuracy = 0.0\n",
    "total_nuanced_accuracy = 0.0\n",
    "\n",
    "for i in range(len(kmers) - 1):\n",
    "    input_kmer = torch.tensor([kmer_to_idx[kmers[i]]], dtype=torch.long)\n",
    "    input_kmer_tensor = one_hot_encode(input_kmer, input_size)\n",
    "    target_kmer = torch.tensor([kmer_to_idx[kmers[i + 1]]], dtype=torch.long)\n",
    "\n",
    "    accuracy, predicted_kmer, actual_kmer = test_model_attention(input_kmer_tensor, target_kmer, model)\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "    nuanced_acc = nuanced_accuracy(predicted_kmer, actual_kmer)\n",
    "    total_nuanced_accuracy += nuanced_acc\n",
    "\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"Input K-mer: {kmers[i]}\")\n",
    "    print(f\"Predicted K-mer: {predicted_kmer}\")\n",
    "    print(f\"Actual K-mer: {actual_kmer}\")\n",
    "    print(f\"Strict Accuracy: {accuracy * 100}%\")\n",
    "    print(f\"Nuanced Accuracy: {nuanced_acc * 100}%\\n\")\n",
    "\n",
    "avg_test_accuracy = total_accuracy / (len(kmers) - 1)\n",
    "avg_nuanced_test_accuracy = total_nuanced_accuracy / (len(kmers) - 1)\n",
    "print(f\"Average strict test accuracy: {avg_test_accuracy * 100:.2f}%\")\n",
    "print(f\"Average nuanced test accuracy: {avg_nuanced_test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
